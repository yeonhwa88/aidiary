{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814f2748",
   "metadata": {},
   "source": [
    "**<font size='5' color='red'> ch01. NLTK 자연어 처리 패키지</font>**\n",
    "# 1. NLTK패키지\n",
    "1. 텍스트 전처리 : 토큰화(어절, 문장 나누기), 정규표현식을 활용한 토큰화, 불용어제거, 기본형(어근)추출\n",
    "2. 품사태깅 : 단어 품사식별\n",
    "3. 구조화된 문서의 빈도수, 분류분석, 연관분석, 감성분석... (단점)NLTK 속도가 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd900613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c17401",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "# c:/nltk_data\n",
    "# d:/nltk_data\n",
    "# e:/nltk_data\n",
    "# c:/Users/내컴퓨터이름/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/share/nltk_data\n",
    "# c:/Users/내컴퓨터이름/anaconda3/lib/nltk_data\n",
    "# c:/Users/내컴퓨터이름/Appdata/Roaming/nltk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ede6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88eec195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치 리스트\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd0d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CH\n"
     ]
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "print(emma[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b46121",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize()\n",
    "# 문장단위로 쪼갠 list\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(emma)\n",
    "print(\"첫 문장: %r\" % (sent_tokens[0]) )\n",
    "print('두번째 문장 : %r' % (sent_tokens[1]))\n",
    "print('문장수 :', len(sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8757b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize() : 단어 단위로 쪼갠 list 반환\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sent_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer클래스 : 토큰화 할 때 정규표현식 이용\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "ret = RegexpTokenizer('\\w+')\n",
    "words = ret.tokenize(sent_tokens[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f690c05",
   "metadata": {},
   "source": [
    "# 2. 형태소(의미있는 가장 작은 말의 단위) 분석\n",
    "- 자연어처리의 기본은 형태소 분석과 품사태깅\n",
    "    * 어간추출(stamming), 원형복원(lemmatizing), 품사태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c16a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['sending', 'cooking', 'files', 'lives', 'crying', 'dying']\n",
    "# 어간추출 (1)\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(words[0]), pst.stem(words[1]), pst.stem(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b83487",
   "metadata": {},
   "outputs": [],
   "source": [
    "[pst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출(2) - ※\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "[lst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출 (3)\n",
    "from nltk.stem import RegexpStemmer\n",
    "rst = RegexpStemmer('ing') # ing를 제거\n",
    "[rst.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간을 추출할 경우 의미가 달라지는 경우 있어 원형복원을 하기도 함 \n",
    "words2 = ['belives', 'cooking']\n",
    "[lst.stem(word) for word in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원형 복원\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "[wl.lemmatize(word) for word in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 : sent_tokens[10]\n",
    "from nltk.tag import pos_tag\n",
    "words = word_tokenize(sent_tokens[9])\n",
    "tagged_list = pos_tag(words)\n",
    "print('단어들 :', words)\n",
    "print('\\n 품사태깅결과 :', tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fec2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사태깅 :  [('Emma', 'NNP'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), ('VOLUME', 'NNP'), ('CHAPTER', 'NNP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), ('handsome', 'VBD'), ('clever', 'NN')]\n",
      "emma 소설의 글자수 :  887071\n",
      "단어출현수 : 123877\n",
      "단어종류수 : 7630\n"
     ]
    }
   ],
   "source": [
    "# 퀴즈 : emma 소설\n",
    "# 1. 특수문자가 들어가지 않은 3글자 이상의 단어만 추출해 품사태깅을 하시오. (RegexpTokenizer)\n",
    "# emma 소설의 글자수, 단어 출현수, 단어종류수, 품사태깅한 데이터 초반 10개 출력\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "ret = RegexpTokenizer('[\\w]{3,}')  # \\w:a-A, +:1글자이상, [\\w]{3}:3글자이상\n",
    "words = ret.tokenize(emma)\n",
    "tagged_list = pos_tag(words)\n",
    "print(\"품사태깅 : \", tagged_list[:10])\n",
    "print('emma 소설의 글자수 : ', len(emma))\n",
    "print('단어출현수 :', len(words))\n",
    "print('단어종류수 :', len(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4eb4feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma 단어수 865\n",
      "품사들 :  {'NNP', 'RB', 'JJ', 'NNS', 'NN', 'NNPS', 'VBD', 'VBP', 'VB', 'VBN'}\n"
     ]
    }
   ],
   "source": [
    "# 2. 'Emma' 단어가 몇 번 등장하며, 품사 태깅이 어떤 품사들로 되어 있는 모두 출력하시오 (NNP, ...)\n",
    "# Emma 단어 출현 횟수, 분류된 품사들 NNP, NNPS\n",
    "count = 0\n",
    "tags = set()\n",
    "\n",
    "# for x , t in tagged_list:\n",
    "#     if x == 'Emma':\n",
    "#         count += 1\n",
    "#         tags.add(t)\n",
    "\n",
    "count = len([x for x, t in tagged_list if x == 'Emma'])\n",
    "tags = set([t for x, t in tagged_list if x == 'Emma'])\n",
    "print('Emma 단어수', count)\n",
    "print('품사들 : ', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77b614b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사가 나온 횟수 : 30781\n",
      "명사가 나온 수 (반복제거) : 4165\n",
      "한단어가 나오는 평균 빈도수 : 7.3903961584633855\n"
     ]
    }
   ],
   "source": [
    "# 3. 내가 원하는 품사 (명사:NN, NNS, NNP, NNPS)의 단어만 뽑아 등장하는 명사의 종류 갯수를 출력하시오\n",
    "# 명사가 나온 횟수 :\n",
    "# 출력한 명사의 수 (반복제거) :\n",
    "# 한단어가 나오는 평균 빈도수 :\n",
    "count = 0\n",
    "dan = set()\n",
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "words = ret.tokenize(emma)\n",
    "tagged_list = pos_tag(words)\n",
    "\n",
    "# for x, m in tagged_list:\n",
    "#     if m == 'NN' or m =='NNS' or m =='NNP' or m =='NNPS':\n",
    "#         count += 1\n",
    "#         dan.add(x)\n",
    "        \n",
    "count = len([x for x, m in tagged_list if m == 'NN' or m =='NNS' or m =='NNP' or m =='NNPS'])\n",
    "dan = len(set([x for x, m in tagged_list if m == 'NN' or m =='NNS' or m =='NNP' or m =='NNPS']))\n",
    "\n",
    "print('명사가 나온 횟수 :', count)\n",
    "print('명사가 나온 수 (반복제거) :' , dan)\n",
    "print('한단어가 나오는 평균 빈도수 :', count/ dan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0f86821",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Text : 단어 리스트와 빈도 분석에서 사용될 클래스\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m emma_text \u001b[38;5;241m=\u001b[39m \u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      7\u001b[0m emma_text\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\text.py:343\u001b[0m, in \u001b[0;36mText.__init__\u001b[1;34m(self, tokens, name)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03mCreate a Text object.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m:param tokens: The source text.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m:type tokens: sequence of str\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_COPY_TOKENS:\n\u001b[1;32m--> 343\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens \u001b[38;5;241m=\u001b[39m tokens\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 명사들 중 최빈단어수\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import Text\n",
    "# Text : 단어 리스트와 빈도 분석에서 사용될 클래스\n",
    "emma_text = Text(count)\n",
    "plt.figure(figsize=(15, 4))\n",
    "emma_text.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08c00d06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emma_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 고유명사(이름) 출현 빈도\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m name_list \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43memma_tags\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNPS\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      3\u001b[0m freq_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m name_list:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emma_tags' is not defined"
     ]
    }
   ],
   "source": [
    "# 고유명사(이름) 출현 빈도\n",
    "name_list = [word for word, tag in emma_tags if tag in ['NNP', 'NNPS']]\n",
    "freq_dict = {}\n",
    "for name in name_list:\n",
    "    if name in freq_dict.keys():\n",
    "        freq_dict[name] += 1\n",
    "    else :\n",
    "        freq_dict[name] = 1\n",
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60e1e5db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[1;32m----> 2\u001b[0m FreqDist(\u001b[43mname_list\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name_list' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "FreqDist(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "614c2fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 150번 이상 반복되는 이름들만 출력\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, freq \u001b[38;5;129;01min\u001b[39;00m FreqDist(\u001b[43mname_list\u001b[49m)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m150\u001b[39m :\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, freq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name_list' is not defined"
     ]
    }
   ],
   "source": [
    "# 150번 이상 반복되는 이름들만 출력\n",
    "for name, freq in FreqDist(name_list).items():\n",
    "    if freq > 150 :\n",
    "        print(name, '-', freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f89b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 내용을 정렬\n",
    "dic = dict(FreqDist(name_list))\n",
    "# dic = freq_dict\n",
    "wordcnt = pd.Series(dic)\n",
    "wordcnt.sort_values(ascending=False, inplace=True)\n",
    "wordcnt.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f72d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150번 이상 출현한 이름을 정렬해서 출력\n",
    "wordcnt[wordcnt>150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
